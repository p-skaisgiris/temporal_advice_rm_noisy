{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrHoJDdE1jaB"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/p-skaisgiris/temporal_advice_rm_noisy.git > /dev/null\n",
        "# !pip install -r temporal_advice_rm_noisy/requirements.txt > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCzqDxnvNUHj"
      },
      "outputs": [],
      "source": [
        "%cd temporal_advice_rm_noisy/\n",
        "!echo $PYTHONPATH\n",
        "%env PYTHONPATH=\"$/env/python:src:src/envs\"\n",
        "!echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train agent without a reward machine\n",
        "!python scripts/train_agent.py --env=Kitchen-v2  --rm-update-algo=no_rm --seed=123 --frames=1000000 --epochs=8 --batch-size=2048 --frames-per-proc=32768 --entropy-coef=0.01 --recurrence=1 --detector-recurrence=1 --detector-epochs=8 --detector-batch-size=2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the learned policy network without RM\n",
        "python scripts/minigrid_visualize.py --env=Kitchen-v2 --model=storage/no_rm-Kitchen-v2-seed123/train/ --rm-update-algo=no_rm --gif=kitchen-no-rm --seed=123 --no-render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBxrQ-qiG2Db"
      },
      "outputs": [],
      "source": [
        "# Collect training data for the detector\n",
        "!python scripts/collect_offline_data.py --env=Kitchen-v2 --seed=111 --episodes=2000\n",
        "%mv collect_Kitchen-v2.pt train_Kitchen-v2.pt\n",
        "!python scripts/collect_offline_data.py --env=Kitchen-v2 --seed=222 --episodes=100\n",
        "%mv collect_Kitchen-v2.pt test_Kitchen-v2.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fbi0ippJ5wk"
      },
      "outputs": [],
      "source": [
        "# Pre-train detector\n",
        "!python scripts/pretrain_detector.py --env=Kitchen-v2 --train-data-path=train_Kitchen-v2.pt --test-data-path=test_Kitchen-v2.pt --use-mem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5glAsz6N2Hr-"
      },
      "outputs": [],
      "source": [
        "# Train agent with tdm and a pretrained detector\n",
        "!python scripts/train_agent.py --env=Kitchen-v2  --rm-update-algo=tdm --seed=123 --frames=1000000 --epochs=8 --batch-size=2048 --frames-per-proc=32768 --entropy-coef=0.01 --recurrence=4 --detector-model=Kitchen-v2_tdm_pretrain_seed1/best --detector-recurrence=4 --detector-epochs=8 --detector-batch-size=2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQFbyYEpp_tS"
      },
      "outputs": [],
      "source": [
        "# Run the learned policy network with pretrained detector TDM and reward machine\n",
        "python scripts/minigrid_visualize.py --env=Kitchen-v2 --model=storage/tdm-r-Kitchen-v2-seed123/train/ --rm-update-algo=tdm --gif=kitchen-tdm-pretrained --seed=123 --use-mem --use-mem-detector --no-render"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
